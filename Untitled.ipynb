{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a variable with the URL to this tutorial\n",
    "url = 'http://www.englishforum.ch/forum.php'\n",
    "# Scrape the HTML at the url\n",
    "r = requests.get(url)\n",
    "# Turn the HTML into a Beautiful Soup object\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business & entrepreneur': 'http://www.englishforum.ch/business-entrepreneur/',\n",
       " 'Commercial events': 'http://www.englishforum.ch/commercial-events/',\n",
       " 'Complaints corner': 'http://www.englishforum.ch/complaints-corner/',\n",
       " 'Concerts': 'http://www.englishforum.ch/concerts/',\n",
       " 'Daily life': 'http://www.englishforum.ch/daily-life/',\n",
       " 'Education': 'http://www.englishforum.ch/education/',\n",
       " 'Employment': 'http://www.englishforum.ch/employment/',\n",
       " 'Entertainment & dining': 'http://www.englishforum.ch/entertainment-dining/',\n",
       " 'Family matters/health': 'http://www.englishforum.ch/family-matters-health/',\n",
       " 'Finance/banking/taxation': 'http://www.englishforum.ch/finance-banking-taxation/',\n",
       " 'Food and drink': 'http://www.englishforum.ch/food-drink/',\n",
       " 'Housing in general': 'http://www.englishforum.ch/housing-general/',\n",
       " 'Insurance': 'http://www.englishforum.ch/insurance/',\n",
       " 'Introductions': 'http://www.englishforum.ch/introductions/',\n",
       " 'Language corner': 'http://www.englishforum.ch/language-corner/',\n",
       " 'Leaving Switzerland': 'http://www.englishforum.ch/leaving-switzerland/',\n",
       " 'Market Place': 'http://www.englishforum.ch/market-place/',\n",
       " 'Off-Topic': 'http://www.englishforum.ch/off-topic/',\n",
       " 'Other/general': 'http://www.englishforum.ch/other-general/',\n",
       " 'Permits/visas/government': 'http://www.englishforum.ch/permits-visas-government/',\n",
       " 'Pet corner': 'http://www.englishforum.ch/pet-corner/',\n",
       " 'Social events': 'http://www.englishforum.ch/social-events/',\n",
       " 'Sports / Fitness / Beauty / Wellness': 'http://www.englishforum.ch/sports-fitness-beauty-wellness/',\n",
       " 'Support': 'http://www.englishforum.ch/support/',\n",
       " 'Swiss news via The Local': 'http://www.englishforum.ch/swiss-news-via-local/',\n",
       " 'Swiss politics/news': 'http://www.englishforum.ch/swiss-politics-news/',\n",
       " 'TV/internet/telephone': 'http://www.englishforum.ch/tv-internet-telephone/',\n",
       " 'Transportation/driving': 'http://www.englishforum.ch/transportation-driving/',\n",
       " 'Travel/day trips/free time': 'http://www.englishforum.ch/travel-day-trips-free-time/'}"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create four variables to score the scraped data in\n",
    "dataframes={}\n",
    "for link in soup.find_all(\"strong\")[1:]:\n",
    "    for linkhref in soup.find_all(\"a\"):\n",
    "        if(link in linkhref and not linkhref['href'].endswith('.html') and link.contents[0] not in dataframes):\n",
    "            dataframes[link.contents[0]]=linkhref['href']\n",
    "\n",
    "# keysToRemove=[]\n",
    "# for value in list(dataframes.values()):\n",
    "#     for index in range(len(dataframes)):\n",
    "#         if(value.endswith('.html')):\n",
    "#             print(value)\n",
    "#         if(value!=list(dataframes.values())[index] and list(dataframes.values())[index].startswith(value)):\n",
    "#             keysToRemove.append(list(dataframes.keys())[index])\n",
    "\n",
    "# print('must remove last-post references',keysToRemove)\n",
    "# print('of')\n",
    "dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in keysToRemove:\n",
    "#     del dataframes[i]\n",
    "# dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.englishforum.ch/other-general/\n",
      "http://www.englishforum.ch/finance-banking-taxation/\n",
      "http://www.englishforum.ch/business-entrepreneur/\n",
      "http://www.englishforum.ch/support/\n",
      "http://www.englishforum.ch/transportation-driving/\n",
      "http://www.englishforum.ch/housing-general/\n",
      "http://www.englishforum.ch/swiss-news-via-local/\n",
      "http://www.englishforum.ch/tv-internet-telephone/\n",
      "http://www.englishforum.ch/off-topic/\n",
      "http://www.englishforum.ch/permits-visas-government/\n",
      "http://www.englishforum.ch/market-place/\n",
      "http://www.englishforum.ch/social-events/\n",
      "http://www.englishforum.ch/family-matters-health/\n",
      "http://www.englishforum.ch/pet-corner/\n",
      "http://www.englishforum.ch/language-corner/\n",
      "http://www.englishforum.ch/employment/\n",
      "http://www.englishforum.ch/entertainment-dining/\n",
      "http://www.englishforum.ch/food-drink/\n",
      "http://www.englishforum.ch/sports-fitness-beauty-wellness/\n",
      "http://www.englishforum.ch/travel-day-trips-free-time/\n",
      "http://www.englishforum.ch/introductions/\n",
      "http://www.englishforum.ch/insurance/\n",
      "http://www.englishforum.ch/daily-life/\n",
      "http://www.englishforum.ch/leaving-switzerland/\n",
      "http://www.englishforum.ch/concerts/\n",
      "http://www.englishforum.ch/education/\n",
      "http://www.englishforum.ch/commercial-events/\n",
      "http://www.englishforum.ch/complaints-corner/\n",
      "http://www.englishforum.ch/swiss-politics-news/\n"
     ]
    }
   ],
   "source": [
    "for i in dataframes:\n",
    "    print(dataframes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4.element import Tag\n",
    "\n",
    "def page(url,index):\n",
    "#     print(url,index)\n",
    "    # Scrape the HTML at the url\n",
    "    r = requests.get(url)    \n",
    "    # Turn the HTML into a Beautiful Soup object\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    subtopic={}\n",
    "    for link in soup.find_all(\"table\",id=\"threadslist\"):\n",
    "    #     print(link)\n",
    "        index_aux=index\n",
    "        dash=0\n",
    "        for replies in link.find_all(\"td\",class_=\"alt1\"):\n",
    "            if(replies.contents[0]!=' '):\n",
    "                if(replies.contents[0]=='-'):\n",
    "                    dash+=1\n",
    "                subtopic[index_aux]=[]\n",
    "                subtopic[index_aux].append(replies.contents[0])\n",
    "# #                 print('rep',index_aux,replies.contents[0])\n",
    "\n",
    "                index_aux+=1\n",
    "        index_aux=index\n",
    "        insertdash=0\n",
    "        for views in link.find_all(\"td\",class_=\"alt2\"):\n",
    "            if(views.contents[0]!=' '):\n",
    "                if(views.contents[0]=='-'):\n",
    "                    if(insertdash==0):\n",
    "                        insertdash=1\n",
    "                    else:\n",
    "                        insertdash=0\n",
    "#                         print('views',index_aux,views.contents[0])\n",
    "                        subtopic[index_aux].append(views.contents[0])\n",
    "                        index_aux+=1\n",
    "                else:\n",
    "#                     print('views',index_aux,views.contents[0])\n",
    "#                 if(index_aux not in subtopic):\n",
    "#                     print('errorrrrrrr',index_aux,views.contents[0])\n",
    "#                     next\n",
    "                    subtopic[index_aux].append(views.contents[0])\n",
    "                    index_aux+=1        \n",
    "#         index_aux=index\n",
    "        for thread in link.find_all(\"a\"):\n",
    "            if('id' in thread.attrs):\n",
    "#                 print('hrefs',index,link1.contents[0])\n",
    "                subtopic[index].append(thread['href'])\n",
    "                subtopic[index].append(thread.contents[0])\n",
    "                index+=1\n",
    "    return [subtopic,index]\n",
    "    \n",
    "    \n",
    "def findAllPages(rootHTML):\n",
    "    # Scrape the HTML at the url\n",
    "    r = requests.get(rootHTML)    \n",
    "    # Turn the HTML into a Beautiful Soup object\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    lastUrl=''\n",
    "    pages=[]\n",
    "    pages.append(rootHTML)\n",
    "    for link in soup.find_all(\"body\"):\n",
    "        for link1 in link.find_all(\"a\"):\n",
    "            if('title' in link1.attrs and 'Last' in link1['title']):\n",
    "                lastUrl=link1['href']\n",
    "                break\n",
    "    if(lastUrl!=''):\n",
    "        limit=re.findall(r'\\d+',lastUrl)[0]\n",
    "        for i in range(2,int(limit)):\n",
    "            pages.append(rootHTML+\"index\"+str(i)+\".html\")\n",
    "        pages.append(lastUrl)\n",
    "        return pages\n",
    "    return None\n",
    "    \n",
    "def createDF(url):\n",
    "#     print(url)\n",
    "    index=0\n",
    "    allsubtopics={}\n",
    "    nextPages=findAllPages(url)\n",
    "    for url in nextPages:\n",
    "#         print('urlllll',url)\n",
    "        result=page(url,index)\n",
    "        allsubtopics.update(result[0])\n",
    "        index=result[1]\n",
    "    #         print('fin',subtopic)\n",
    "        replies=[]\n",
    "        views=[]\n",
    "        hrefs=[]\n",
    "        threads=[]\n",
    "        \n",
    "    for value in allsubtopics.values():\n",
    "        replies.append(value[0])\n",
    "        views.append(value[1])\n",
    "        hrefs.append(value[2])\n",
    "        threads.append(value[3])\n",
    "    df = pd.DataFrame({'replies': replies, 'views': views, 'hrefs': hrefs, 'threads' : threads})\n",
    "    return df\n",
    "\n",
    "# for index,i in enumerate(dataframes):\n",
    "#     if(index==1):\n",
    "#         df=createDF(dataframes[i])\n",
    "#         print(df)\n",
    "#     print(dataframes[i])\n",
    "other_general=createDF(list(dataframes.values())[0])\n",
    "other_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def damn(url):\n",
    "#     print(url,index)\n",
    "    # Scrape the HTML at the url\n",
    "    r = requests.get(url)    \n",
    "    # Turn the HTML into a Beautiful Soup object\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    subtopic={}\n",
    "    # len(elems.find_all(\"a\"))\n",
    "    for link in soup.find_all(\"table\",id=\"threadslist\"):\n",
    "    #     print(link)\n",
    "#         index_aux=index\n",
    "        dash=0\n",
    "        for replies in link.find_all(\"td\",class_=\"alt1\"):\n",
    "#             if(type(replies.contents[0]) is not Tag and replies.contents[0].isdigit()):\n",
    "            if(replies.contents[0]!=' '):\n",
    "                if(replies.contents[0]=='-'):\n",
    "                    dash+=1\n",
    "                print(replies.contents[0])\n",
    "        insertdash=0\n",
    "        for views in link.find_all(\"td\",class_=\"alt2\"):\n",
    "            if(views.contents[0]!=' '):\n",
    "                if(views.contents[0]=='-'):\n",
    "                    if(insertdash==0):\n",
    "                        insertdash=1\n",
    "                    else:\n",
    "                        insertdash=0\n",
    "                        print(views.contents[0])\n",
    "                else:\n",
    "                    print(views.contents[0])\n",
    "        for hrefs in link.find_all(\"a\"):\n",
    "            if('id' in hrefs.attrs):\n",
    "                print(hrefs.contents[0])\n",
    "# damn(list(dataframes.values())[0]+\"index9.html\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
