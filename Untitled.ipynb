{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4.element import Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a variable with the URL to this tutorial\n",
    "url = 'http://www.englishforum.ch/forum.php'\n",
    "# Scrape the HTML at the url\n",
    "r = requests.get(url)\n",
    "# Turn the HTML into a Beautiful Soup object\n",
    "soup = BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business & entrepreneur': 'http://www.englishforum.ch/business-entrepreneur/',\n",
       " 'Commercial events': 'http://www.englishforum.ch/commercial-events/',\n",
       " 'Complaints corner': 'http://www.englishforum.ch/complaints-corner/',\n",
       " 'Concerts': 'http://www.englishforum.ch/concerts/',\n",
       " 'Daily life': 'http://www.englishforum.ch/daily-life/',\n",
       " 'Education': 'http://www.englishforum.ch/education/',\n",
       " 'Employment': 'http://www.englishforum.ch/employment/',\n",
       " 'Entertainment & dining': 'http://www.englishforum.ch/entertainment-dining/',\n",
       " 'Family matters/health': 'http://www.englishforum.ch/family-matters-health/',\n",
       " 'Finance/banking/taxation': 'http://www.englishforum.ch/finance-banking-taxation/',\n",
       " 'Food and drink': 'http://www.englishforum.ch/food-drink/',\n",
       " 'Housing in general': 'http://www.englishforum.ch/housing-general/',\n",
       " 'Insurance': 'http://www.englishforum.ch/insurance/',\n",
       " 'Introductions': 'http://www.englishforum.ch/introductions/',\n",
       " 'Language corner': 'http://www.englishforum.ch/language-corner/',\n",
       " 'Leaving Switzerland': 'http://www.englishforum.ch/leaving-switzerland/',\n",
       " 'Market Place': 'http://www.englishforum.ch/market-place/',\n",
       " 'Off-Topic': 'http://www.englishforum.ch/off-topic/',\n",
       " 'Other/general': 'http://www.englishforum.ch/other-general/',\n",
       " 'Permits/visas/government': 'http://www.englishforum.ch/permits-visas-government/',\n",
       " 'Pet corner': 'http://www.englishforum.ch/pet-corner/',\n",
       " 'Social events': 'http://www.englishforum.ch/social-events/',\n",
       " 'Sports / Fitness / Beauty / Wellness': 'http://www.englishforum.ch/sports-fitness-beauty-wellness/',\n",
       " 'Support': 'http://www.englishforum.ch/support/',\n",
       " 'Swiss news via The Local': 'http://www.englishforum.ch/swiss-news-via-local/',\n",
       " 'Swiss politics/news': 'http://www.englishforum.ch/swiss-politics-news/',\n",
       " 'TV/internet/telephone': 'http://www.englishforum.ch/tv-internet-telephone/',\n",
       " 'Transportation/driving': 'http://www.englishforum.ch/transportation-driving/',\n",
       " 'Travel/day trips/free time': 'http://www.englishforum.ch/travel-day-trips-free-time/'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create four variables to score the scraped data in\n",
    "dataframes={}\n",
    "for link in soup.find_all(\"strong\")[1:]:\n",
    "    for linkhref in soup.find_all(\"a\"):\n",
    "        if(link in linkhref and not linkhref['href'].endswith('.html') and link.contents[0] not in dataframes):\n",
    "            dataframes[link.contents[0]]=linkhref['href']\n",
    "dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch the info from urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def comments(thread):\n",
    "    # Scrape the HTML at the url\n",
    "    response = requests.get(thread,headers={'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'})\n",
    "    # Turn the HTML into a Beautiful Soup object\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    comments={}\n",
    "    for link in soup.find_all(\"div\",{\"align\":\"center\"}):\n",
    "        for link2 in link.find_all(\"div\",{\"align\":\"left\"}):\n",
    "            for link3 in link2.find_all(\"tr\"):\n",
    "                for link4 in link3.find_all(\"td\",{\"width\":\"99%\"}):\n",
    "                    for link5 in link4.find_all(\"div\"):\n",
    "                        if('id' in link5.attrs and link5['id'].startswith('post_message')):\n",
    "#                             print(link5.attrs,link5.contents[0])\n",
    "                            contents=''\n",
    "                            for content in link5.contents:\n",
    "                                if type(content) is not Tag:\n",
    "                                    contents+=' '+content\n",
    "                            comments[link5['id']]=contents\n",
    "#     print(comments)\n",
    "    return ''.join(comments.values())\n",
    "\n",
    "def page(url,index):\n",
    "#     print(url,index)\n",
    "    # Scrape the HTML at the url\n",
    "    r = requests.get(url)    \n",
    "    # Turn the HTML into a Beautiful Soup object\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    subtopic={}\n",
    "    for link in soup.find_all(\"table\",id=\"threadslist\"):\n",
    "#         print(link)\n",
    "        index_aux=index\n",
    "        dash=0\n",
    "        for replies in link.find_all(\"td\",class_=\"alt1\"):\n",
    "            if(replies.contents[0]!=' ' and type(replies.contents[0])!=Tag):\n",
    "                if(replies.contents[0]=='-'):\n",
    "                    dash+=1\n",
    "                subtopic[index_aux]=[]\n",
    "                subtopic[index_aux].append(replies.contents[0])\n",
    "# #                 print('rep',index_aux,replies.contents[0])\n",
    "\n",
    "                index_aux+=1\n",
    "    \n",
    "#         print('Replies found:',index_aux)\n",
    "        index_aux=index\n",
    "        insertdash=0\n",
    "        # \\xa0 Unicode --> ascii 160\n",
    "        for views in link.find_all(\"td\",class_=\"alt2\"):\n",
    "            if(views.contents[0]!=' ' and ord(views.contents[0][0])!=160 and type(views.contents[0])!=Tag):\n",
    "                if(views.contents[0]=='-'):\n",
    "                    if(insertdash==0):\n",
    "                        insertdash=1\n",
    "                    else:\n",
    "                        insertdash=0\n",
    "                        subtopic[index_aux].append(views.contents[0])\n",
    "                        index_aux+=1\n",
    "                else:\n",
    "                    subtopic[index_aux].append(views.contents[0])\n",
    "                    index_aux+=1    \n",
    "#         print('Views found:',index_aux)\n",
    "        for thread in link.find_all(\"a\"):\n",
    "            if('id' in thread.attrs):\n",
    "#                 print('hrefs',index,link1.contents[0])\n",
    "                    subtopic[index].append(thread['href'])\n",
    "                    subtopic[index].append(thread.contents[0])\n",
    "                    index+=1\n",
    "        \n",
    "#         print('Threads found:',index)\n",
    "    return [subtopic,index]\n",
    "    \n",
    "    \n",
    "def findAllPages(rootHTML):\n",
    "    # Scrape the HTML at the url\n",
    "    r = requests.get(rootHTML)    \n",
    "    # Turn the HTML into a Beautiful Soup object\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    lastUrl=''\n",
    "    pages=[]\n",
    "    pages.append(rootHTML)\n",
    "    for link in soup.find_all(\"body\"):\n",
    "        for link1 in link.find_all(\"a\"):\n",
    "            if('title' in link1.attrs and 'Last' in link1['title']):\n",
    "                lastUrl=link1['href']\n",
    "                print(link1['href'])\n",
    "                break\n",
    "    if(lastUrl==''):#no Last page link, we fetch last index page\n",
    "        links=set()\n",
    "        for link in soup.find_all(\"body\"):\n",
    "            for link1 in link.find_all(\"a\"):\n",
    "                if('title' in link1.attrs and 'Show results' in link1['title']):\n",
    "                    links.add(link1['href'])\n",
    "#                     print(link1)\n",
    "        if(rootHTML!=sorted(links)[-1]):\n",
    "            lastUrl=sorted(links)[-1]\n",
    "    if(lastUrl!=''):\n",
    "        if(rootHTML.endswith('.html')):\n",
    "            limit=re.findall(r'\\d+',lastUrl)[-1]\n",
    "            for i in range(2,int(limit)):\n",
    "                pages.append(re.sub(r'.html','',rootHTML)+\"-\"+str(i)+\".html\")\n",
    "        elif('members' in rootHTML):\n",
    "            limit=re.findall(r'\\d+',lastUrl)[0]\n",
    "            lastSlash=rootHTML.rfind('/')\n",
    "            for i in range(2,int(limit)):\n",
    "                pages.append(rootHTML[:lastSlash]+\"/index\"+str(i)+\".html\"+rootHTML[lastSlash+1:])\n",
    "        else:\n",
    "            limit=re.findall(r'\\d+',lastUrl)[0]\n",
    "            for i in range(2,int(limit)):\n",
    "                pages.append(rootHTML+\"index\"+str(i)+\".html\")\n",
    "        pages.append(lastUrl)\n",
    "        return pages\n",
    "    return pages\n",
    "    \n",
    "def createDF(url):\n",
    "#     print(url)\n",
    "    index=0\n",
    "    allsubtopics={}\n",
    "    nextPages=findAllPages(url)\n",
    "    for url in nextPages:\n",
    "#         print('urlllll',url)\n",
    "        result=page(url,index)\n",
    "        allsubtopics.update(result[0])\n",
    "        index=result[1]\n",
    "    #         print('fin',subtopic)\n",
    "    replies=[]\n",
    "    views=[]\n",
    "    hrefs=[]\n",
    "    threads=[]\n",
    "    for value in allsubtopics.values():\n",
    "        replies.append(value[0])\n",
    "        views.append(value[1])\n",
    "        hrefs.append(value[2])\n",
    "        threads.append(value[3])\n",
    "    posts=[]\n",
    "    for href in hrefs:\n",
    "        nextPages=findAllPages(href)\n",
    "        threadPosts=[]\n",
    "        for page in nextPages:\n",
    "            threadPosts.append(comments(page))\n",
    "        posts.append(''.join(threadPosts))\n",
    "    df = pd.DataFrame({'replies': replies, 'views': views, 'hrefs': hrefs, 'threads' : threads, 'posts': posts})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data into csv folder and remove dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveData(dataframe):\n",
    "    dataframe.to_csv(\"csv/\"+i.replace(\"/\",\"\")+\".csv\", sep='\\t', encoding='utf-8',)\n",
    "    del dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes we'd like to fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Business & entrepreneur http://www.englishforum.ch/business-entrepreneur/\n",
      "1 Commercial events http://www.englishforum.ch/commercial-events/\n",
      "2 Complaints corner http://www.englishforum.ch/complaints-corner/\n",
      "3 Concerts http://www.englishforum.ch/concerts/\n",
      "4 Daily life http://www.englishforum.ch/daily-life/\n",
      "5 Education http://www.englishforum.ch/education/\n",
      "6 Employment http://www.englishforum.ch/employment/\n",
      "7 Entertainment & dining http://www.englishforum.ch/entertainment-dining/\n",
      "8 Family matters/health http://www.englishforum.ch/family-matters-health/\n",
      "9 Finance/banking/taxation http://www.englishforum.ch/finance-banking-taxation/\n",
      "10 Food and drink http://www.englishforum.ch/food-drink/\n",
      "11 Housing in general http://www.englishforum.ch/housing-general/\n",
      "12 Insurance http://www.englishforum.ch/insurance/\n",
      "13 Introductions http://www.englishforum.ch/introductions/\n",
      "14 Language corner http://www.englishforum.ch/language-corner/\n",
      "15 Leaving Switzerland http://www.englishforum.ch/leaving-switzerland/\n",
      "16 Market Place http://www.englishforum.ch/market-place/\n",
      "17 Off-Topic http://www.englishforum.ch/off-topic/\n",
      "18 Other/general http://www.englishforum.ch/other-general/\n",
      "19 Permits/visas/government http://www.englishforum.ch/permits-visas-government/\n",
      "20 Pet corner http://www.englishforum.ch/pet-corner/\n",
      "21 Social events http://www.englishforum.ch/social-events/\n",
      "22 Sports / Fitness / Beauty / Wellness http://www.englishforum.ch/sports-fitness-beauty-wellness/\n",
      "23 Support http://www.englishforum.ch/support/\n",
      "24 Swiss news via The Local http://www.englishforum.ch/swiss-news-via-local/\n",
      "25 Swiss politics/news http://www.englishforum.ch/swiss-politics-news/\n",
      "26 TV/internet/telephone http://www.englishforum.ch/tv-internet-telephone/\n",
      "27 Transportation/driving http://www.englishforum.ch/transportation-driving/\n",
      "28 Travel/day trips/free time http://www.englishforum.ch/travel-day-trips-free-time/\n"
     ]
    }
   ],
   "source": [
    "# for i in dataframes:\n",
    "for index,i in enumerate(sorted(dataframes)):\n",
    "    print(index,i,dataframes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In order to read from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readData(filename):\n",
    "    if( not filename.endswith('.csv')):\n",
    "        filename+='.csv'\n",
    "    df = pd.read_csv('csv/'+filename, header=0,sep='\\t',index_col=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV files we have got so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Commercial events.csv\n",
      "1 Education.csv\n",
      "2 Social events.csv\n",
      "3 Swiss news via The Local.csv\n",
      "4 business_enterpreteur.csv\n",
      "5 complaints_corner.csv\n",
      "6 concerts.csv\n",
      "7 daily_life.csv\n",
      "8 employment.csv\n",
      "9 entertainment_dining.csv\n",
      "10 family_matters_health.csv\n",
      "11 finance_banking_taxation.csv\n",
      "12 food_drink.csv\n",
      "13 housing.csv\n",
      "14 insurance.csv\n",
      "15 introductions.csv\n",
      "16 language_corner.csv\n",
      "17 leaving_switzerland.csv\n",
      "18 market_place.csv\n",
      "19 off_topic.csv\n",
      "20 other_general.csv\n",
      "21 permits_visas_gov.csv\n",
      "22 pet_corner.csv\n",
      "23 rm\n",
      "24 sports_wellness.csv\n",
      "25 support.csv\n",
      "26 swiss_politics_news.csv\n",
      "27 transportation_driving.csv\n",
      "28 travel_free_time.csv\n",
      "29 tv_internet_telephone.csv\n"
     ]
    }
   ],
   "source": [
    "for index,i in enumerate(sorted(os.listdir(\"csv/\"))):\n",
    "    print(index,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In case we'd like to update the files, let's declare the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business_enterpreteur=None\n",
    "complaints_corner=None\n",
    "concerts=None\n",
    "daily_life=None\n",
    "employment=None\n",
    "entertainment_dining=None\n",
    "family_matters_health=None\n",
    "finance_banking_taxation=None\n",
    "food_drink=None\n",
    "housing=None\n",
    "insurance=None\n",
    "introductions=None\n",
    "language_corner=None\n",
    "leaving_switzerland=None\n",
    "market_place=None\n",
    "off_topic=None\n",
    "other_general=None\n",
    "permits_visas_gov=None\n",
    "pet_corner=None\n",
    "sports_wellness=None\n",
    "support=None\n",
    "swiss_politics_news=None\n",
    "transportation_driving=None\n",
    "travel_free_time=None\n",
    "tv_internet_telephone=None\n",
    "# NON FETCHED BUT YET TO COME\n",
    "commercial_events=None\n",
    "education=None\n",
    "social_events=None\n",
    "swiss_news=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Let's initialize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 http://www.englishforum.ch/business-entrepreneur/\n",
      "1 http://www.englishforum.ch/commercial-events/\n",
      "2 http://www.englishforum.ch/complaints-corner/\n",
      "3 http://www.englishforum.ch/concerts/\n",
      "4 http://www.englishforum.ch/daily-life/\n",
      "5 http://www.englishforum.ch/education/\n",
      "6 http://www.englishforum.ch/employment/\n",
      "7 http://www.englishforum.ch/entertainment-dining/\n",
      "8 http://www.englishforum.ch/family-matters-health/\n",
      "9 http://www.englishforum.ch/finance-banking-taxation/\n",
      "10 http://www.englishforum.ch/food-drink/\n",
      "11 http://www.englishforum.ch/housing-general/\n",
      "12 http://www.englishforum.ch/insurance/\n",
      "13 http://www.englishforum.ch/introductions/\n",
      "14 http://www.englishforum.ch/language-corner/\n",
      "15 http://www.englishforum.ch/leaving-switzerland/\n",
      "16 http://www.englishforum.ch/market-place/\n",
      "17 http://www.englishforum.ch/off-topic/\n",
      "18 http://www.englishforum.ch/other-general/\n",
      "19 http://www.englishforum.ch/permits-visas-government/\n",
      "20 http://www.englishforum.ch/pet-corner/\n",
      "21 http://www.englishforum.ch/social-events/\n",
      "22 http://www.englishforum.ch/sports-fitness-beauty-wellness/\n",
      "23 http://www.englishforum.ch/support/\n",
      "24 http://www.englishforum.ch/swiss-news-via-local/\n",
      "Replies found: 30\n",
      "Views found: 30\n",
      "Threads found: 30\n",
      "\n",
      "\n",
      "printinggggggg\n",
      " {0: ['171', '55,553', 'http://www.englishforum.ch/swiss-news-via-local/126982-svp-immigrants-don-t-mess-swiss-flag.html', 'SVP to immigrants: Don\\x92t mess with the Swiss flag'], 1: ['36', '9,162', 'http://www.englishforum.ch/swiss-news-via-local/151583-allow-porn-schools-swiss-men-s-group.html', \"Allow porn in schools: Swiss men's group\"], 2: ['4', '2,222', 'http://www.englishforum.ch/swiss-news-via-local/148345-google-wins-swiss-street-view-privacy-appeal.html', 'Google wins Swiss Street View privacy appeal'], 3: ['49', '9,983', 'http://www.englishforum.ch/swiss-news-via-local/142396-swatch-plans-car-fueled-hydrogen-oxygen.html', 'Swatch plans car fueled by hydrogen and oxygen'], 4: ['32', '6,675', 'http://www.englishforum.ch/swiss-news-via-local/131644-eu-aiding-greece-dig-out-unpaid-taxes-swiss-banks.html', 'EU aiding Greece to dig out unpaid taxes in Swiss banks'], 5: ['93', '19,855', 'http://www.englishforum.ch/swiss-news-via-local/151193-kill-stray-cats-swiss-environment-official.html', 'Kill stray cats: Swiss environment official'], 6: ['21', '10,815', 'http://www.englishforum.ch/swiss-news-via-local/151300-workers-find-gold-bars-swiss-bushes.html', 'Workers find gold bars in Swiss bushes'], 7: ['8', '31,098', 'http://www.englishforum.ch/swiss-news-via-local/137242-prosecutors-arrest-swiss-club-xamax-s-owner.html', \"Prosecutors arrest Swiss club Xamax's owner\"], 8: ['27', '61,037', 'http://www.englishforum.ch/swiss-news-via-local/139523-seventh-infant-found-swiss-baby-hatch.html', \"Seventh infant found in Swiss 'baby hatch'\"], 9: ['36', '10,045', 'http://www.englishforum.ch/swiss-news-via-local/125411-swiss-make-tv-licence-fee-compulsory.html', 'Swiss to make TV licence fee compulsory'], 10: ['15', '4,724', 'http://www.englishforum.ch/swiss-news-via-local/142704-mafia-members-moving-switzerland.html', 'Mafia members moving to Switzerland'], 11: ['5', '2,998', 'http://www.englishforum.ch/swiss-news-via-local/149179-thousands-march-zurich-gay-pride-festival.html', 'Thousands march at Zurich gay pride festival'], 12: ['45', '8,738', 'http://www.englishforum.ch/swiss-news-via-local/130000-central-bank-posts-gains-high-gold-prices.html', 'Central bank posts gains on high gold prices'], 13: ['1', '4,797', 'http://www.englishforum.ch/swiss-news-via-local/151655-swiss-cops-face-charges-over-injured-cat.html', 'Swiss cops face charges over injured cat'], 14: ['12', '10,149', 'http://www.englishforum.ch/swiss-news-via-local/140726-christian-democrats-want-asylum-seeker-villages.html', \"Christian Democrats want asylum seeker 'villages'\"], 15: ['30', '6,412', 'http://www.englishforum.ch/swiss-news-via-local/150966-test-arafat-poisoning-abbas-swiss-experts.html', 'Test Arafat for poisoning: Abbas to Swiss experts'], 16: ['7', '31,684', 'http://www.englishforum.ch/swiss-news-via-local/148850-tough-asylum-laws-contrary-tradition.html', \"Tough asylum laws 'contrary to tradition'\"], 17: ['1', '2,883', 'http://www.englishforum.ch/swiss-news-via-local/129392-swiss-seize-millions-czech-energy-case.html', 'Swiss seize millions in Czech energy case'], 18: ['74', '15,980', 'http://www.englishforum.ch/swiss-news-via-local/128485-canton-zurich-cuts-access-international-schools.html', 'Canton Zurich cuts access to international schools'], 19: ['23', '8,027', 'http://www.englishforum.ch/swiss-news-via-local/151663-sleepy-zurich-boozy-valais-swiss-holiday-habits-revealed.html', 'Sleepy Zurich, boozy Valais: Swiss holiday habits revealed'], 20: ['10', '4,324', 'http://www.englishforum.ch/swiss-news-via-local/126442-swiss-slap-fresh-sanctions-syria.html', 'Swiss slap fresh sanctions on Syria'], 21: ['24', '6,804', 'http://www.englishforum.ch/swiss-news-via-local/131469-zurich-residents-vote-drive-sex-stalls.html', 'Zurich residents to vote on drive-in sex stalls'], 22: ['41', '8,774', 'http://www.englishforum.ch/swiss-news-via-local/148740-swiss-mull-tough-benefit-cuts-asylum-seekers.html', 'Swiss mull tough benefit cuts for asylum seekers'], 23: ['14', '4,631', 'http://www.englishforum.ch/swiss-news-via-local/146499-swiss-bank-wegelin-fights-us-summons.html', 'Swiss bank Wegelin fights US summons'], 24: ['69', '14,075', 'http://www.englishforum.ch/swiss-news-via-local/137172-zurich-bans-minors-sex-trade.html', 'Zurich bans minors from the sex trade'], 25: ['18', '4,604', 'http://www.englishforum.ch/swiss-news-via-local/150391-asylum-seekers-should-shot-svp-politician.html', 'Asylum seekers should be shot: SVP politician'], 26: ['77', '12,491', 'http://www.englishforum.ch/swiss-news-via-local/141524-swiss-edge-closer-gay-adoption.html', 'Swiss edge closer to gay adoption'], 27: ['25', '8,965', 'http://www.englishforum.ch/swiss-news-via-local/133693-nestl-best-employer-switzerland-survey.html', 'Nestlé best employer in Switzerland: survey'], 28: ['27', '6,972', 'http://www.englishforum.ch/swiss-news-via-local/122814-swiss-train-police-carry-guns.html', 'Swiss train police to carry guns'], 29: ['6', '4,940', 'http://www.englishforum.ch/swiss-news-via-local/130668-sexual-assistants-stigma-helping-disabled-swiss.html', 'Sexual assistants - the stigma of helping disabled Swiss']}\n",
      "25 http://www.englishforum.ch/swiss-politics-news/\n",
      "26 http://www.englishforum.ch/tv-internet-telephone/\n",
      "27 http://www.englishforum.ch/transportation-driving/\n",
      "28 http://www.englishforum.ch/travel-day-trips-free-time/\n"
     ]
    }
   ],
   "source": [
    "for index,i in enumerate(sorted(dataframes)):\n",
    "    print(index, dataframes[i])\n",
    "    df=createDF(dataframes[i])\n",
    "    saveData(df)\n",
    "#     if index==0:  \n",
    "#         business_enterpreteur=createDF(dataframes[i])\n",
    "#         saveData(business_enterpreteur)\n",
    "#     elif index==1:\n",
    "#         commercial_events=createDF(dataframes[i])#####OK\n",
    "#         saveData(commercial_events)\n",
    "#     elif index==2: \n",
    "#         complaints_corner=createDF(dataframes[i])   \n",
    "#         saveData(complaints_corner)\n",
    "#     elif index==3: \n",
    "#         concerts=createDF(dataframes[i])\n",
    "#         saveData(concerts)        \n",
    "#     elif index==4: \n",
    "#         daily_life=createDF(dataframes[i])\n",
    "#         saveData(daily_life)\n",
    "#     elif index==5: \n",
    "#         education=createDF(dataframes[i])#####OK\n",
    "#         saveData(education)\n",
    "#     elif index==6: \n",
    "#         employment=createDF(dataframes[i])\n",
    "#         saveData(employment)\n",
    "#     elif index==7: \n",
    "#         entertainment_dining=createDF(dataframes[i])\n",
    "#         saveData(entertainment_dining)\n",
    "#     elif index==8: \n",
    "#         family_matters_health=createDF(dataframes[i])\n",
    "#         saveData(family_matters_health)\n",
    "#     elif index==9: \n",
    "#         finance_banking_taxation=createDF(dataframes[i])\n",
    "#         saveData(finance_banking_taxation)\n",
    "#     elif index==10: \n",
    "#         food_drink=createDF(dataframes[i])\n",
    "#         saveData(food_drink)\n",
    "#     elif index==11: \n",
    "#         housing=createDF(dataframes[i])\n",
    "#         saveData(housing)\n",
    "#     elif index==12: \n",
    "#         insurance=createDF(dataframes[i])\n",
    "#         saveData(insurance)\n",
    "#     elif index==13: \n",
    "#         introductions=createDF(dataframes[i])\n",
    "#         saveData(introductions)\n",
    "#     elif index==14: \n",
    "#         language_corner=createDF(dataframes[i])\n",
    "#         saveData(language_corner)\n",
    "#     elif index==15: \n",
    "#         leaving_switzerland=createDF(dataframes[i])\n",
    "#         saveData(leaving_switzerland)\n",
    "#     elif index==16: \n",
    "#         market_place=createDF(dataframes[i])\n",
    "#         saveData(market_place)\n",
    "#     elif index==17: \n",
    "#         off_topic=createDF(dataframes[i])\n",
    "#         saveData(off_topic)\n",
    "#     elif index==18: \n",
    "#         other_general=createDF(dataframes[i])\n",
    "#         saveData(other_general)\n",
    "#     elif index==19: \n",
    "#         permits_visas_gov=createDF(dataframes[i])\n",
    "#         saveData(permits_visas_gov)\n",
    "#     elif index==20: \n",
    "#         pet_corner=createDF(dataframes[i])\n",
    "#         saveData(pet_corner)\n",
    "#     elif index==21: \n",
    "#         social_events=createDF(dataframes[i])#####OK\n",
    "#         saveData(social_events)\n",
    "#     elif index==22: \n",
    "#         sports_wellness=createDF(dataframes[i])\n",
    "#         saveData(sports_wellness)\n",
    "#     elif index==23: \n",
    "#         support=createDF(dataframes[i])\n",
    "#         saveData(support)\n",
    "#     elif index==24: \n",
    "#         swiss_news=createDF(dataframes[i])######\n",
    "#         saveData(swiss_news)\n",
    "#     elif index==25: \n",
    "#         swiss_politics_news=createDF(dataframes[i])\n",
    "#         saveData(swiss_politics_news)\n",
    "#     elif index==26: \n",
    "#         tv_internet_telephone=createDF(dataframes[i])\n",
    "#         saveData(tv_internet_telephone)\n",
    "#     elif index==27: \n",
    "#         transportation_driving=createDF(dataframes[i])\n",
    "#         saveData(transportation_driving)\n",
    "#     else: \n",
    "#         travel_free_time=createDF(dataframes[i])\n",
    "#         saveData(travel_free_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all data at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "dfContainer={}#sorted(dataframes)\n",
    "filenames = glob.glob(\"csv/*.csv\")\n",
    "for index,filename in enumerate(filenames):\n",
    "    df = pd.read_csv(filename, header=0,sep='\\t',index_col=0)\n",
    "    dfContainer[re.sub(r'.csv','',os.path.basename(filename))]=df\n",
    "\n",
    "# sorted(dataframes)\n",
    "# dataFrames = pd.Series(dataFrames)\n",
    "# df=pd.read_csv('Concerts',sep='\\t',index_col=0)\n",
    "# df\n",
    "\n",
    "# dfContainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas\n",
    "- Buscar comentarios más positivos y encontrar los threads de más interes\n",
    "\n",
    "- con los threads más visitados podemos obtener los topics de más interes\n",
    "- los usuarios que más aportan en el foro\n",
    "- los temas más polemicos (tal vez usando los comentarios negativos)\n",
    "- En un apartado de ventas que hay, intentar sacar lo que se vende en suiza y en donde (aunque el donde no lo tengo muy claro)\n",
    "- las horas de más publicaciones\n",
    "- lodgement: tal vez podemos aplicar ML como un clasificador de oportunidades de encontrar casa en suiza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def comments(thread):\n",
    "#     # Scrape the HTML at the url\n",
    "#     response = requests.get(thread,headers={'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'})\n",
    "#     # Turn the HTML into a Beautiful Soup object\n",
    "#     soup = BeautifulSoup(response.text, 'lxml')\n",
    "#     comments={}\n",
    "#     for link in soup.find_all(\"div\",{\"align\":\"center\"}):\n",
    "#         for link2 in link.find_all(\"div\",{\"align\":\"left\"}):\n",
    "#             for link3 in link2.find_all(\"tr\"):\n",
    "#                 for link4 in link3.find_all(\"td\",{\"width\":\"99%\"}):\n",
    "#                     for link5 in link4.find_all(\"div\"):\n",
    "#                         if('id' in link5.attrs and link5['id'].startswith('post_message')):\n",
    "# #                             print(link5.attrs,link5.contents[0])\n",
    "#                             contents=''\n",
    "#                             for content in link5.contents:\n",
    "#                                 if type(content) is not Tag:\n",
    "#                                     contents+=' '+content\n",
    "#                             comments[link5['id']]=contents\n",
    "# #     print(comments)\n",
    "#     return ''.join(comments.values())\n",
    "# comments('http://www.englishforum.ch/education/263961-need-your-help.html')\n",
    "# findAllPages('http://www.englishforum.ch/business-enterpreteur/')\n",
    "# findAllPages('http://www.englishforum.ch/business-entrepreneur/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Users list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.englishforum.ch/members/list/index348.html?pp=100&order=asc&sort=joindate\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def fetchUsers():\n",
    "    url='http://www.englishforum.ch/members/list/?pp=100&order=asc&sort=joindate'\n",
    "    nextPages=findAllPages(url)\n",
    "#     nextPages=[url]\n",
    "    userslinks={}\n",
    "    for page in nextPages:\n",
    "        response = requests.get(nextPages[0])\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        for link in soup.find_all(\"form\"):\n",
    "            for link1 in link.find_all('table'):\n",
    "                for link1 in link.find_all('a'):\n",
    "                    if('href' in link1.attrs and 'members' in link1['href'] and not 'list/' in link1['href']):\n",
    "                        userslinks[link1.contents[0]]=link1['href']\n",
    "    return userslinks\n",
    "# fetchUsers()\n",
    "def fetchUsersDetails(url):\n",
    "#     url='http://www.englishforum.ch/members/1-mark.html'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    user={}\n",
    "    for link in soup.find_all('div',id='usercss'):\n",
    "#         print(link)\n",
    "        for link1 in link.find_all('a'):\n",
    "#                 print(link1)\n",
    "            if('href' in link1.attrs and 'do=finduser' in link1['href']):\n",
    "                if('starteronly' in link1['href']):\n",
    "                    user['threads']=link1['href']\n",
    "                else:\n",
    "                    user['posts']=link1['href']\n",
    "        fields=[]            \n",
    "        info=[]\n",
    "        for link1 in link.find_all('dt'):\n",
    "            if('Page' not in link1.contents[0]):\n",
    "                fields.append(link1.contents[0])\n",
    "        for link1 in link.find_all('dd'):\n",
    "            if(type(link1.contents[0])!=Tag):\n",
    "                info.append(link1.contents[0])\n",
    "        for i in range(len(info)):\n",
    "            user[fields[i]]=info[i]\n",
    "    url=user['threads']\n",
    "#     print(url)\n",
    "    num=0\n",
    "    first_thread='http://www.englishforum.ch/'\n",
    "    first_post=first_thread\n",
    "    response=requests.get(url)\n",
    "    soup=BeautifulSoup(response.text,'lxml')\n",
    "    for link in soup.find_all('form',id='inlinemodform'):\n",
    "        for link1 in link.find_all('table'):\n",
    "            if('id' in link1.attrs):\n",
    "                for link2 in link1.find_all('a'):\n",
    "                    if('href' in link2.attrs and 'id' in link2.attrs):\n",
    "#                         print(link2)\n",
    "                        num+=1\n",
    "                        first_thread+=link2['href']\n",
    "                        if(num==2):\n",
    "                            break\n",
    "    if(num==2):\n",
    "        first_thread='Undetermined'\n",
    "    user['first_thread']=first_thread\n",
    "    url=user['posts']\n",
    "    print(url)\n",
    "    time.sleep(4.5)\n",
    "    response=requests.get(url,headers={'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'})\n",
    "    soup=BeautifulSoup(response.text,'lxml')\n",
    "    for link in soup.find_all('table'):\n",
    "#         print(link)\n",
    "        for link1 in link.find_all('table'):\n",
    "            if('id' in link1.attrs and 'post' in link1['id']):\n",
    "#                 print(link1)\n",
    "                for link2 in link1.find_all('a'):\n",
    "#                     print(link2)\n",
    "                    if('href' in link2.attrs and 'post' in link2['href']):\n",
    "                        first_post+=link2['href']\n",
    "                        break\n",
    "    user['first_post']=first_post[:first_post.find('.html')+5]\n",
    "    url=user['first_post']\n",
    "    postMessage='post_message_'+re.findall(r'\\d+',url)[-1]\n",
    "    time.sleep(4.5)\n",
    "    response=requests.get(url,headers={'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'})\n",
    "    soup=BeautifulSoup(response.text,'lxml')\n",
    "    for link in soup.find_all('div',id=postMessage):\n",
    "        user['first_post']=link.contents[0]\n",
    "        break\n",
    "#     print(soup)\n",
    "#     print(num)\n",
    "    return user\n",
    "# fetchUsersDetails()\n",
    "users=fetchUsers()\n",
    "for i in users:\n",
    "    users[i]=fetchUsersDetails(users[i])\n",
    "    print(i,users[i])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
